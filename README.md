# Multimodal-ML
Deep Learning project that demonstrates how to use OpenAI's CLIP model for zero-shot image classification using multimodal inputs (image + natural language prompts). It includes embedding visualization using PCA and works out of the box with a sample image and text labels.
